---

airflow_python_local_dist_packages: '/usr/local/lib/python3.5/dist-packages'
airflow_python_libraries_path: '/usr/lib/python3/dist/packages/:/usr/local/lib/python3.5/dist-packages/'

# We assume that MRI database is deployed under the same Postgres instance as airflow database
# Identifier must be under /data-factory/airflow namespace to be able to use dependencies on scheduler and webserver
airflow_db_marathon_id: '/data-factory/airflow/airflow-db'

airflow_scheduler_marathon_id: '/data-factory/airflow/scheduler'
airflow_scheduler_marathon_dependencies:
  - "{{ airflow_db_marathon_id }}"

airflow_webserver_marathon_id: '/data-factory/airflow/webserver'
airflow_webserver_marathon_dependencies:
  - "{{ airflow_db_marathon_id }}"
  - "{{ airflow_scheduler_marathon_id }}"

# TODO airflow_executor: 'MesosExecutor'

# Need both Python2 version of the Postgres library for ansible and Python 3 version for Airflow
airflow_db_deb_packages:
  - python-pip
  - python-psycopg2
  - python3-pip
  - python3-psycopg2
  - python3-numpy
  - curl

mri_db_deb_packages:
  - python3-pip
  - python3-psycopg2
  - curl

airflow_deb_packages:
  - netcat
  - curl
  - python3-pip
  - python3-dev
  - python3-psycopg2
  - libffi-dev
  - build-essential
  - locales
  - libffi-dev
  - libssl-dev

airflow_pip_packages:
  - "pytz==2015.7"
  - pyOpenSSL
  - ndg-httpsclient
  - pyasn1
  - pydicom
  - alembic
  - "slackclient=={{ slackclient_py_version }}"
  - "docker-py=={{ docker_py_version }}"
  - "mri-meta-extract=={{ mri_meta_extract_version }}"
  - "i2b2-import=={{ i2b2_import_version }}"
  - "airflow_imaging_plugins=={{ airflow_imaging_plugins_version }}"

airflow_plugins:
  - name: mesos
    required_debs: []
    required_pips: []
  - name: postgresql
    required_debs:
      - postgresql-client
      - python3-psycopg2
    required_pips: []
  - name: crypto
    required_debs:
      - libssl-dev
      - python-cryptography
      - python3-cryptography
    required_pips: []
  - name: ldap
    required_debs:
      - libkrb5-dev
      - libsasl2-dev
    required_pips: []

#airflow_pool_io_intensive: 12
#airflow_pool_remote_file_copy: 2
#airflow_pool_image_preprocessing: '{{ ansible_processor_vcpus - airflow_pool_io_intensive - 1 }}'

# List of slot pools to create in Airflow (similar to using the Pools menu in Airflow webserver)
airflow_slot_pools:
  - name: 'image_preprocessing'
    slots: '{{ airflow_pool_image_preprocessing }}'
    description: 'Number of parallel Matlab instances that can process images'
  - name: 'io_intensive'
    slots: '{{ airflow_pool_io_intensive }}'
    description: 'Number of concurrent IO intensive processes that can run in parallel'
  - name: 'remote_file_copy'
    slots: '{{ airflow_pool_remote_file_copy }}'
    description: 'Number of concurrent file copy operations over the network'

# airflow_scripts_verify_commit
airflow_scripts:
  - url: https://github.com/HBPMedical/mri-preprocessing-pipeline.git
    version: '{{ mri_preprocessing_pipeline_version }}'
    verify_commit: '{{ airflow_scripts_verify_commit | default(False) }}'
    dest: '{{ airflow_scripts_root }}/mri-preprocessing-pipeline'
  - url: https://github.com/HBPMedical/data-factory-airflow-dags.git
    version: '{{data_factory_airflow_dags_version }}'
    # TODO set to true
    verify_commit: false
    dest: '{{ airflow_scripts_root }}/data-factory-airflow-dags'

airflow_dags_folder: "{{ airflow_scripts_root }}/data-factory-airflow-dags"

airflow_extra_settings:
  - section: mesos_env
    parameters:
      PYTHONPATH: '{{ airflow_python_libraries_path }}'
  - section: spm
    parameters:
      spm_dir: '{{ spm_dir }}'
  - section: "data-factory"
    parameters:
      sql_alchemy_conn: '{{ data_catalog_db_sql_alchemy_conn }}'
      datasets: '{{ data_factory_datasets }}'
      email_errors_to: '{{ data_factory_email_errors_to }}'
      slack_token: '{{ data_factory_slack_token | default(slack_token) }}'
      slack_channel: '{{ data_factory_slack_channel }}'
      slack_channel_user: '{{ data_factory_slack_channel_user }}'
      mipmap_db_confile_file: "{{ data_factory_mipmap_db_confile_file | default('/dev/null') }}"
  # MAIN
  - section: "data-factory:main"
    parameters:
      dataset_label: '{{ main_dataset_label }}'
  # PROPROCESSING
  - section: "data-factory:main:preprocessing"
    parameters:
      input_folder: '{{ main_preprocessing_input_folder }}'
      input_config: '{{ main_preprocessing_input_config }}'
      max_active_runs: '{{ main_preprocessing_max_active_runs }}'
      min_free_space: '{{ main_preprocessing_min_free_space }}'
      scanners: '{{ main_preprocessing_scanners }}'
      pipelines: '{{ main_preprocessing_pipelines }}'
      pipelines_path: '{{ main_preprocessing_pipelines_path }}'
      protocols_file: '{{ main_preprocessing_protocols_file }}'
      nifti_local_folder: '{{ main_preprocessing_nifti_local_folder }}'
      nifti_server_folder: "{{ main_preprocessing_nifti_server_folder | default(None) }}"
  - section: "data-factory:main:preprocessing:copy-to-local"
    parameters:
      output_folder: "{{ main_preprocessing_copy_to_local_output_folder }}"
  - section: "data-factory:main:preprocessing:dicom_organiser"
    parameters:
      docker_image: "{{ main_preprocessing_dicom_organiser_docker_image | default('hbpmip/hierarchizer') }}:{{ main_preprocessing_dicom_organiser_version | default('latest') }}"
      docker_input_dir: "{{ main_preprocessing_dicom_organiser_input_folder | default(None) }}"
      docker_output_dir: "{{ main_preprocessing_dicom_organiser_output_folder | default(None) }}"
      data_structure: "{{ main_preprocessing_dicom_organiser_data_structure }}"
      output_folder: "{{ main_preprocessing_dicom_organiser_output_folder }}"
  - section: "data-factory:main:preprocessing:nifti_organiser"
    parameters:
      docker_image: "{{ main_preprocessing_nifti_organiser_docker_image | default('hbpmip/hierarchizer') }}:{{ main_preprocessing_nifti_organiser_version | default('latest') }}"
      docker_input_dir: "{{ main_preprocessing_nifti_organiser_input_folder | default(None) }}"
      docker_output_dir: "{{ main_preprocessing_nifti_organiser_output_folder | default(None) }}"
      data_structure: "{{ main_preprocessing_nifti_organiser_data_structure }}"
      output_folder: "{{ main_preprocessing_nifti_organiser_output_folder }}"
  - section: "data-factory:main:preprocessing:images_selection"
    parameters:
      output_folder: "{{ main_preprocessing_images_selection_output_folder | default(None) }}"
      images_selection_csv_path: "{{ main_preprocessing_images_selection_file_path | default(None) }}"
  - section: "data-factory:main:preprocessing:dicom_select_T1"
    parameters:
      output_folder: "{{ main_preprocessing_dicom_select_t1_output_folder | default(None) }}"
      protocols_definition_file: "{{ main_preprocessing_dicom_select_T1_protocols_definition_file }}"
      spm_function: "{{ main_preprocessing_dicom_select_t1_spm_function }}"
      pipeline_path: "{{ main_preprocessing_dicom_select_t1_pipeline_path }}"
      misc_library_path: "{{ main_preprocessing_dicom_select_t1_misc_library_path }}"
      backup_folder: "{{ main_preprocessing_dicom_select_t1_backup_folder }}"
      dcm2nii_program: "{{ main_preprocessing_dicom_select_t1_dcm2nii_program }}"
  - section: "data-factory:main:preprocessing:mpm_maps"
    parameters:
      output_folder: '{{ main_preprocessing_mpm_maps_output_folder }}'
      server_folder: "{{ main_preprocessing_mpm_maps_server_folder | default(None) }}"
  - section: "data-factory:main:preprocessing:neuro_morphometric_atlas"
    parameters:
      output_folder: '{{ main_preprocessing_neuro_morphometric_atlas_local_folder }}'
      server_folder: "{{ main_preprocessing_neuro_morphometric_atlas_server_folder | default(None) }}"
      TPM_template: "{{ main_preprocessing_neuro_morphometric_atlas_TPM_template | default(spm_dir + '/tpm/nwTPM_sl3.nii') }}"
      spm_function: "{{ main_preprocessing_neuro_morphometric_atlas_spm_function }}"
  - section: "data-factory:main:preprocessing:features_to_i2b2"
    parameters:
      output_folder: "{{ main_preprocessing_features_to_i2b2_local_folder | default(None) }}"
  # ETL
  - section: "data-factory:main:etl"
    parameters:
      min_free_space: "{{ main_etl_min_free_space }}"
  - section: "data-factory:main:etl:ehr_to_i2b2"
    parameters:
      scanners: "{{ main_preprocessing_ehr_scanners | default('') }}"
      data_folder: '{{ main_preprocessing_ehr_data_folder | default(None) }}'
      data_folder_depth: '{{ main_preprocessing_ehr_data_folder_depth | default(1) }}'
      docker_image: '{{ main_preprocessing_ehr_to_i2b2_capture_docker_image | default(None) }}'
      output_folder: '{{ main_preprocessing_ehr_to_i2b2_capture_folder | default(None) }}'
  - section: "data-factory:main:etl:version_ehr"
    parameters:
      output_folder: '{{ main_etl_version_ehr_output_folder | default(None) }}'

airflow_work_dirs: "
  {% set dirs = [] %}
  {% if main_preprocessing_dicom_organizer_local_folder is defined %}
    {% set dirs = dirs + [main_preprocessing_dicom_organizer_local_folder] %}
  {% endif %}
  {% if main_preprocessing_images_selection_local_folder is defined %}
    {% set dirs = dirs + [main_preprocessing_images_selection_local_folder] %}
  {% endif %}
  {% if main_preprocessing_dicom_select_T1_local_folder is defined %}
    {% set dirs = dirs + [main_preprocessing_dicom_select_T1_local_folder] %}
  {% endif %}
  {{ dirs | list }}
"
